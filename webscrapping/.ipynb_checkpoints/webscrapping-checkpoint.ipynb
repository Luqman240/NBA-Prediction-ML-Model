{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d019619",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import asyncio\n",
    "import requests\n",
    "\n",
    "DATA_DIR = 'data'\n",
    "SEASONS = range(2014,2024)\n",
    "STANDINGS_DIR = os.path.join(DATA_DIR, 'standings')\n",
    "SCORES_DIR = os.path.join(DATA_DIR, 'scores')\n",
    "BASE_URL = 'https://www.basketball-reference.com/'\n",
    "loop = asyncio.get_event_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c2b6be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range(2014, 2024)\n"
     ]
    }
   ],
   "source": [
    "print(SEASONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2ee7a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GET HTML ELEMENTS BY THEIR SELECTORS\n",
    "def get_html(url, selector, duration=5, retries=3):\n",
    "    html = None\n",
    "    \n",
    "    for i in range(1, retries+1):\n",
    "        time.sleep(duration * i)\n",
    "        try:\n",
    "            browser = webdriver.Chrome()\n",
    "            browser.get(url)  # Utilisez browser.get(url) pour accéder à l'URL\n",
    "            print(browser.title)\n",
    "            elements = browser.find_elements(By.CSS_SELECTOR,selector)\n",
    "            html = [element.get_attribute(\"innerHTML\") for element in elements]\n",
    "        except Exception as e:\n",
    "            print(f\"Error on attempt {i}: {str(e)}\")\n",
    "            continue\n",
    "        else:\n",
    "            break\n",
    "        finally:\n",
    "            browser.quit()\n",
    "        \n",
    "    return html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5988abdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013-14 NBA Schedule | Basketball-Reference.com\n",
      "2014-15 NBA Schedule | Basketball-Reference.com\n",
      "2015-16 NBA Schedule | Basketball-Reference.com\n",
      "2016-17 NBA Schedule | Basketball-Reference.com\n",
      "2017-18 NBA Schedule | Basketball-Reference.com\n",
      "2018-19 NBA Schedule | Basketball-Reference.com\n",
      "2019-20 NBA Schedule | Basketball-Reference.com\n",
      "2020-21 NBA Schedule | Basketball-Reference.com\n",
      "2021-22 NBA Schedule | Basketball-Reference.com\n",
      "2022-23 NBA Schedule | Basketball-Reference.com\n"
     ]
    }
   ],
   "source": [
    "#Get all season period link\n",
    "filter_links = {}\n",
    "for season in SEASONS:\n",
    "    url = f\"https://www.basketball-reference.com/leagues/NBA_{season}_games.html\"\n",
    "    filter_elements = get_html(url, '.filter', 5,3)\n",
    "    soup = BeautifulSoup(('').join(filter_elements), 'html.parser')\n",
    "    #print(soup)\n",
    "    links = [link.get('href') for link in soup.find_all(\"a\")]\n",
    "    #print(links)\n",
    "    filter_links[season] = links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b4dab51",
   "metadata": {},
   "outputs": [],
   "source": [
    "box_score_links = {}\n",
    "def get_box_scores(links):\n",
    "    if(not os.path.exists(f\"./BOXSCORES\")):\n",
    "        os.makedirs(f\"./BOXSCORES\")\n",
    "    for season in SEASONS:\n",
    "        file_path = f\"./BOXSCORES/BOXSCORES_{season}.txt\"\n",
    "        if os.path.exists(file_path):\n",
    "            continue\n",
    "        box_score_links[season] = []\n",
    "        for link in filter_links[season]:\n",
    "            box_scores = get_html(BASE_URL+link, '[data-stat=\"box_score_text\"]')\n",
    "            soup = BeautifulSoup(''.join(box_scores), 'html.parser')\n",
    "            box_score_links[season] = box_score_links[season] + [element.get('href') for element in soup.find_all('a')]\n",
    "        with open(file_path, 'w') as file:\n",
    "        # Loop through the list and write each string to the file\n",
    "            for string in box_score_links[season]:\n",
    "                file.write(string + '\\n') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dbded496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013-14 NBA Schedule | Basketball-Reference.com\n",
      "2013-14 NBA Schedule | Basketball-Reference.com\n",
      "2013-14 NBA Schedule | Basketball-Reference.com\n",
      "2013-14 NBA Schedule | Basketball-Reference.com\n",
      "2013-14 NBA Schedule | Basketball-Reference.com\n",
      "2013-14 NBA Schedule | Basketball-Reference.com\n",
      "2013-14 NBA Schedule | Basketball-Reference.com\n",
      "2013-14 NBA Schedule | Basketball-Reference.com\n",
      "2013-14 NBA Schedule | Basketball-Reference.com\n",
      "2014-15 NBA Schedule | Basketball-Reference.com\n",
      "2014-15 NBA Schedule | Basketball-Reference.com\n",
      "2014-15 NBA Schedule | Basketball-Reference.com\n",
      "2014-15 NBA Schedule | Basketball-Reference.com\n",
      "2014-15 NBA Schedule | Basketball-Reference.com\n",
      "2014-15 NBA Schedule | Basketball-Reference.com\n",
      "2014-15 NBA Schedule | Basketball-Reference.com\n",
      "2014-15 NBA Schedule | Basketball-Reference.com\n",
      "2014-15 NBA Schedule | Basketball-Reference.com\n",
      "2015-16 NBA Schedule | Basketball-Reference.com\n",
      "2015-16 NBA Schedule | Basketball-Reference.com\n",
      "2015-16 NBA Schedule | Basketball-Reference.com\n",
      "2015-16 NBA Schedule | Basketball-Reference.com\n",
      "2015-16 NBA Schedule | Basketball-Reference.com\n",
      "2015-16 NBA Schedule | Basketball-Reference.com\n",
      "2015-16 NBA Schedule | Basketball-Reference.com\n",
      "2015-16 NBA Schedule | Basketball-Reference.com\n",
      "2015-16 NBA Schedule | Basketball-Reference.com\n",
      "2016-17 NBA Schedule | Basketball-Reference.com\n",
      "2016-17 NBA Schedule | Basketball-Reference.com\n",
      "2016-17 NBA Schedule | Basketball-Reference.com\n",
      "2016-17 NBA Schedule | Basketball-Reference.com\n",
      "2016-17 NBA Schedule | Basketball-Reference.com\n",
      "2016-17 NBA Schedule | Basketball-Reference.com\n",
      "2016-17 NBA Schedule | Basketball-Reference.com\n",
      "2016-17 NBA Schedule | Basketball-Reference.com\n",
      "2016-17 NBA Schedule | Basketball-Reference.com\n",
      "2017-18 NBA Schedule | Basketball-Reference.com\n",
      "2017-18 NBA Schedule | Basketball-Reference.com\n",
      "2017-18 NBA Schedule | Basketball-Reference.com\n",
      "2017-18 NBA Schedule | Basketball-Reference.com\n",
      "2017-18 NBA Schedule | Basketball-Reference.com\n",
      "2017-18 NBA Schedule | Basketball-Reference.com\n",
      "2017-18 NBA Schedule | Basketball-Reference.com\n",
      "2017-18 NBA Schedule | Basketball-Reference.com\n",
      "2017-18 NBA Schedule | Basketball-Reference.com\n",
      "2018-19 NBA Schedule | Basketball-Reference.com\n",
      "2018-19 NBA Schedule | Basketball-Reference.com\n",
      "2018-19 NBA Schedule | Basketball-Reference.com\n",
      "2018-19 NBA Schedule | Basketball-Reference.com\n",
      "2018-19 NBA Schedule | Basketball-Reference.com\n",
      "2018-19 NBA Schedule | Basketball-Reference.com\n",
      "2018-19 NBA Schedule | Basketball-Reference.com\n",
      "2018-19 NBA Schedule | Basketball-Reference.com\n",
      "2018-19 NBA Schedule | Basketball-Reference.com\n",
      "2019-20 NBA Schedule | Basketball-Reference.com\n",
      "2019-20 NBA Schedule | Basketball-Reference.com\n",
      "2019-20 NBA Schedule | Basketball-Reference.com\n",
      "2019-20 NBA Schedule | Basketball-Reference.com\n",
      "2019-20 NBA Schedule | Basketball-Reference.com\n",
      "2019-20 NBA Schedule | Basketball-Reference.com\n",
      "2019-20 NBA Schedule | Basketball-Reference.com\n",
      "2019-20 NBA Schedule | Basketball-Reference.com\n",
      "2019-20 NBA Schedule | Basketball-Reference.com\n",
      "2019-20 NBA Schedule | Basketball-Reference.com\n",
      "2020-21 NBA Schedule | Basketball-Reference.com\n",
      "2020-21 NBA Schedule | Basketball-Reference.com\n",
      "2020-21 NBA Schedule | Basketball-Reference.com\n",
      "2020-21 NBA Schedule | Basketball-Reference.com\n",
      "2020-21 NBA Schedule | Basketball-Reference.com\n",
      "2020-21 NBA Schedule | Basketball-Reference.com\n",
      "2020-21 NBA Schedule | Basketball-Reference.com\n",
      "2020-21 NBA Schedule | Basketball-Reference.com\n",
      "2021-22 NBA Schedule | Basketball-Reference.com\n",
      "2021-22 NBA Schedule | Basketball-Reference.com\n",
      "2021-22 NBA Schedule | Basketball-Reference.com\n",
      "2021-22 NBA Schedule | Basketball-Reference.com\n",
      "2021-22 NBA Schedule | Basketball-Reference.com\n",
      "2021-22 NBA Schedule | Basketball-Reference.com\n",
      "2021-22 NBA Schedule | Basketball-Reference.com\n",
      "2021-22 NBA Schedule | Basketball-Reference.com\n",
      "2021-22 NBA Schedule | Basketball-Reference.com\n",
      "2022-23 NBA Schedule | Basketball-Reference.com\n",
      "2022-23 NBA Schedule | Basketball-Reference.com\n",
      "2022-23 NBA Schedule | Basketball-Reference.com\n",
      "2022-23 NBA Schedule | Basketball-Reference.com\n",
      "2022-23 NBA Schedule | Basketball-Reference.com\n",
      "2022-23 NBA Schedule | Basketball-Reference.com\n",
      "2022-23 NBA Schedule | Basketball-Reference.com\n",
      "2022-23 NBA Schedule | Basketball-Reference.com\n",
      "2022-23 NBA Schedule | Basketball-Reference.com\n"
     ]
    }
   ],
   "source": [
    "get_box_scores(filter_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7418eb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_key_from_value(dictionary, value):\n",
    "    for key, val in dictionary.items():\n",
    "        if val == value:\n",
    "            return key\n",
    "    # If the value is not found, you can handle it as needed, e.g., return None or raise an exception.\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad0c5886",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEAMS = {\n",
    "    \"ATL\": 0,\n",
    "    \"BOS\": 1,\n",
    "    \"BKN\": 2,\n",
    "    \"CHA\": 3,\n",
    "    \"CHI\": 4,\n",
    "    \"CLE\": 5,\n",
    "    \"DAL\": 6,\n",
    "    \"DEN\": 7,\n",
    "    \"DET\": 8,\n",
    "    \"GSW\": 9,\n",
    "    \"HOU\": 10,\n",
    "    \"IND\": 11,\n",
    "    \"LAC\": 12,\n",
    "    \"LAL\": 13,\n",
    "    \"MEM\": 14,\n",
    "    \"MIA\": 15,\n",
    "    \"MIL\": 16,\n",
    "    \"MIN\": 17,\n",
    "    \"NOP\": 18,\n",
    "    \"NYK\": 19,\n",
    "    \"OKC\": 20,\n",
    "    \"ORL\": 21,\n",
    "    \"PHI\": 22,\n",
    "    \"PHX\": 23,\n",
    "    \"POR\": 24,\n",
    "    \"SAC\": 25,\n",
    "    \"SAS\": 26,\n",
    "    \"TOR\": 27,\n",
    "    \"UTA\": 28,\n",
    "    \"WAS\": 29\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28f033f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cf91fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_nan(df):\n",
    "    for i in range(len(df)):\n",
    "        for j in range(len(df.iloc[i])):\n",
    "            if(df.iloc[i,j] == \"Did Not Play\" or df.iloc[i,j] == \"Did Not Dress\" or df.iloc[i,j] == \"Not With Team\"):\n",
    "                df.iloc[i,j] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf383b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Line Score Table to get the name of team\n",
    "def get_team_df(link):\n",
    "    line_score = get_html(BASE_URL+link, \"[id='div_line_score']\")\n",
    "    soup_line_score = BeautifulSoup(line_score[0], 'html.parser')\n",
    "    line_score_table = soup_line_score.find('table')\n",
    "    pd_line_score = pd.read_html(StringIO(line_score_table.prettify()))\n",
    "    line_score_df = pd_line_score[0]\n",
    "    team_names = np.array(line_score_df.iloc[:,0])\n",
    "    line_score_df\n",
    "    won = False\n",
    "    if(int(line_score_df.iloc[0, -1]) > int(line_score_df.iloc[1,-1])):\n",
    "        won = True\n",
    "    else:\n",
    "        won = False\n",
    "\n",
    "    team_names = np.append(team_names,won)\n",
    "    team_names = team_names.reshape(1, -1)\n",
    "    team_names_header = ['TEAM', 'TEAM_OPP', 'WON']\n",
    "    team_row = pd.DataFrame(team_names, columns=team_names_header)\n",
    "    return team_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a758613b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links_from_file(filepath):\n",
    "    with open(filepath, 'r') as file:\n",
    "        lines = [line.strip() for line in file.readlines()]\n",
    "        return lines\n",
    "\n",
    "def dataframe_processing(df):\n",
    "    str_to_nan(df)\n",
    "    df = df.drop(5)\n",
    "    df = df.sort_index(axis=1)\n",
    "    df.pop(df.columns[-1])\n",
    "    df.pop(df.columns[0])\n",
    "    column_names = [t[1] for t in df.columns]\n",
    "    max_column_names = [c+'_MAX' for c in column_names]\n",
    "    column_names += max_column_names\n",
    "    values = np.array(df.iloc[-1])\n",
    "    df.drop(df.tail(1).index,inplace=True)\n",
    "    return df, values\n",
    "\n",
    "\n",
    "\n",
    "def add_max_values(df):\n",
    "    temp_df = df\n",
    "    temp_df = temp_df.apply(pd.to_numeric, errors='coerce')\n",
    "    #Delete rows with only NAN values\n",
    "    temp_df = temp_df.dropna(axis=1, how='all')\n",
    "    #print(temp_df)\n",
    "    values = np.nanmax(temp_df, axis=0)\n",
    "    return values\n",
    "\n",
    "def get_header(df):\n",
    "    headers = [t[1] for t in df.columns]\n",
    "    return headers\n",
    "\n",
    "def get_max_header(df):\n",
    "    headers = []\n",
    "    for t in df.columns:\n",
    "        s = t[1] +'_MAX'\n",
    "        headers.append(s)\n",
    "    headers.pop(13)\n",
    "    return headers\n",
    "\n",
    "def get_opp_header(df):\n",
    "    headers = ['OPP_'+t[1] for t in df.columns]\n",
    "    return headers\n",
    "\n",
    "def get_opp_max_header(df):\n",
    "    headers = []\n",
    "    for t in df.columns:\n",
    "        s = 'OPP_'+t[1] +'_MAX'\n",
    "        headers.append(s)\n",
    "    headers.pop(13)\n",
    "    return headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "050780c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_html_files(season):\n",
    "    if(not os.path.exists(\"./HTML_FILES\")):\n",
    "        os.makedirs(f\"./HTML_FILES\")\n",
    "    print(f\"!!! DOWNLOADING SEASON {season} !!!\")\n",
    "    time.sleep(10)\n",
    "    if(not os.path.exists(f\"./HTML_FILES/{season}\")):\n",
    "        os.makedirs(f\"./HTML_FILES/{season}\")\n",
    "    season_dataframe = pd.DataFrame()\n",
    "    boxscore_links = get_links_from_file(f'BOXSCORES/BOXSCORES_{season}.txt')\n",
    "    for j in range(len(boxscore_links)):\n",
    "        temp_link = boxscore_links[j].replace(\"/boxscores/\", \"\")\n",
    "        filename = temp_link.replace(\".html\", \"\")\n",
    "        if(os.path.exists(f\"./HTML_FILES/{season}/{filename}.html\")):\n",
    "            print(\"existe deja\")\n",
    "            continue\n",
    "        time.sleep(2)\n",
    "        response = requests.get(BASE_URL + boxscore_links[j])\n",
    "        with open(f\"./HTML_FILES/{season}/{filename}.html\", 'wb') as file:\n",
    "            file.write(response.content)\n",
    "        \"\"\"#GET HTML TABLES\n",
    "        tables_html = get_html(BASE_URL+link, \"[class='section_content is_setup']\")\n",
    "        if(len(tables_html) != 4):\n",
    "            continue\n",
    "        soup_table = BeautifulSoup(tables_html[0], 'html.parser')\n",
    "        basic_stat = soup_table.find('table')\n",
    "\n",
    "        opp_soup_table = BeautifulSoup(tables_html[2], 'html.parser')\n",
    "        opp_basic_stat = soup_table.find('table')\n",
    "\n",
    "        table = pd.read_html(StringIO(basic_stat.prettify()))[0]\n",
    "        opp_table = pd.read_html(StringIO(basic_stat.prettify()))[0]\n",
    "\n",
    "        #GET DATAFRAMES\n",
    "        df, row_values = dataframe_processing(table)\n",
    "        opp_df, opp_row_values = dataframe_processing(opp_table)\n",
    "\n",
    "\n",
    "        #FIRST TEAM\n",
    "        max_values = add_max_values(df)\n",
    "        header = get_header(df)\n",
    "        max_header = get_max_header(df)\n",
    "        max_values = max_values.reshape(1, -1)\n",
    "        row_values = row_values.reshape(1, -1)\n",
    "\n",
    "        row = pd.DataFrame(row_values, columns=header)\n",
    "        max_row = pd.DataFrame(max_values, columns=max_header)\n",
    "\n",
    "        #OPP TEAM\n",
    "        opp_max_values = add_max_values(opp_df)\n",
    "        opp_header = get_opp_header(opp_df)\n",
    "        opp_max_header = get_opp_max_header(opp_df)\n",
    "        opp_max_values = opp_max_values.reshape(1, -1)\n",
    "        opp_row_values = opp_row_values.reshape(1, -1)\n",
    "\n",
    "        opp_row = pd.DataFrame(opp_row_values, columns=opp_header)\n",
    "        opp_max_row = pd.DataFrame(opp_max_values, columns=opp_max_header)\n",
    "\n",
    "        match_row = get_team_df(link)\n",
    "\n",
    "        full_row = pd.concat([row,opp_row,max_row,opp_max_row,match_row], axis=1)\n",
    "        season_dataframe = pd.concat([season_dataframe, full_row], ignore_index=True)\"\"\"\n",
    "        #temp[season] = season_dataframe\n",
    "    #return temp\n",
    "\n",
    "def get_dataframes(season):\n",
    "    #for season in my_range:\n",
    "    directory_path = f\"./HTML_FILES/{season}\"\n",
    "    #if not os.path.exists(directory_path):\n",
    "        #return\n",
    "\n",
    "    \n",
    "    #for filename in filenames:\n",
    "    with open(directory_path+'/201410280NOP.html', 'r', encoding='utf-8') as file:\n",
    "        html_content = file.read()\n",
    "\n",
    "    tables_html = BeautifulSoup(html_content, 'html.parser')\n",
    "    #soup_tables = tables_html.find_all('div', class_=\"is_setup\")\n",
    "    print(tables_html.find_all(attrs={'class':'stats_table'}))\n",
    "    \"\"\"soup_table = tables_html[0]\n",
    "    basic_stat = soup_table.find('table')\n",
    "\n",
    "    opp_soup_table = tables_html[2]\n",
    "    opp_basic_stat = soup_table.find('table')\n",
    "\n",
    "    table = pd.read_html(StringIO(basic_stat.prettify()))[0]\n",
    "    opp_table = pd.read_html(StringIO(basic_stat.prettify()))[0]\n",
    "\n",
    "    #GET DATAFRAMES\n",
    "    df, row_values = dataframe_processing(table)\n",
    "    opp_df, opp_row_values = dataframe_processing(opp_table)\n",
    "\n",
    "\n",
    "    #FIRST TEAM\n",
    "    max_values = add_max_values(df)\n",
    "    header = get_header(df)\n",
    "    max_header = get_max_header(df)\n",
    "    max_values = max_values.reshape(1, -1)\n",
    "    row_values = row_values.reshape(1, -1)\n",
    "\n",
    "    row = pd.DataFrame(row_values, columns=header)\n",
    "    max_row = pd.DataFrame(max_values, columns=max_header)\n",
    "\n",
    "    #OPP TEAM\n",
    "    opp_max_values = add_max_values(opp_df)\n",
    "    opp_header = get_opp_header(opp_df)\n",
    "    opp_max_header = get_opp_max_header(opp_df)\n",
    "    opp_max_values = opp_max_values.reshape(1, -1)\n",
    "    opp_row_values = opp_row_values.reshape(1, -1)\n",
    "\n",
    "    opp_row = pd.DataFrame(opp_row_values, columns=opp_header)\n",
    "    opp_max_row = pd.DataFrame(opp_max_values, columns=opp_max_header)\n",
    "\n",
    "    match_row = get_team_df(link)\n",
    "\n",
    "    full_row = pd.concat([row,opp_row,max_row,opp_max_row,match_row], axis=1)\n",
    "    season_dataframe = pd.concat([season_dataframe, full_row], ignore_index=True)\n",
    "    print(season_dataframe)\"\"\"\n",
    "    \n",
    "def export_dataframes(dataframes):\n",
    "    for key in dataframes:\n",
    "        if(not os.path.exists(f\"../DATAS\")):\n",
    "            os.makedirs(f\"../DATAS\")\n",
    "        if os.path.exists(f\"../DATAS/season_{key}\"):\n",
    "            continue\n",
    "        dataframes[key].to_excel(f'../DATAS/season_{key}.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f1cdcd84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "get_dataframes(2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8e0db5df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!! DOWNLOADING SEASON 2023 !!!\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "existe deja\n",
      "!!! DOWNLOAD FOR SEASON 2022 DONE !!!\n",
      "!!! WAIT FOR SEASON 2023 !!!\n"
     ]
    }
   ],
   "source": [
    "get_all_html_files(2023)\n",
    "print(f\"!!! DOWNLOAD FOR SEASON 2022 DONE !!!\")\n",
    "print(f\"!!! WAIT FOR SEASON 2023 !!!\")\n",
    "time.sleep(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929f993b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
